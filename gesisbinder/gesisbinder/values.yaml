etcJupyter:
  jupyter_notebook_config.json:
    NotebookApp:
      allow_origin: '*'
      tornado_settings:
        trust_xheaders: true
      # shutdown the server after no activity
      shutdown_no_activity_timeout: 600

    # if a user leaves a notebook with a running kernel,
    # the effective idle timeout will typically be CULL_TIMEOUT + CULL_KERNEL_TIMEOUT
    # as culling the kernel will register activity,
    # resetting the no_activity timer for the server as a whole
    # Check also jupyterhub.cull.timeout config
    MappingKernelManager:
      # shutdown kernels after no activity
      cull_idle_timeout: 600
      # check for idle kernels this often
      cull_interval: 60
      # a kernel with open connections but no activity still counts as idle
      # this is what allows us to shutdown servers
      # when people leave a notebook open and wander off
      cull_connected: true

# this is set in _secret.yaml
# this is required to send GESIS Binder events to mybinder.org's StackDriver
eventsArchiver:
  serviceAccountKey: ""

binderhub:
  pdb:
    minAvailable: 1
  replicas: 2
  config:
    GitHubRepoProvider:
      # Add banned repositories to the list below
      # They should be strings that will match "^<org-name>/<repo-name>.*"
      banned_specs:
        # e.g. '^org/repo.*'
        - ^ines/spacy-binder.*
        - ^soft4voip/rak.*
        - ^hmharshit/cn-ait.*
        - ^shishirchoudharygic/mltraining.*
        - ^hmharshit/mltraining.*
      high_quota_specs:
        - ^gesiscss/.*
      #spec_config:
      #  - pattern: ^gesiscss/.*
      #    config:
      #      quota: 200
    BinderHub:
      pod_quota: 200
      base_url: /binder/
      use_registry: true
      build_image: jupyter/repo2docker:0.10.0-125.g94d492e
      build_node_selector:
        user: worker
      per_repo_quota: 100
      per_repo_quota_higher: 200

      template_path: /etc/binderhub/templates
      # look at configmap for static files here https://discourse.jupyter.org/t/customizing-jupyterhub-on-kubernetes/1769/4?u=bitniks
      #extra_static_path: /etc/binderhub/custom/gesisbinder/static
      #extra_static_url_prefix: /extra_static/
      template_variables:
        version: beta
        home_url: /
        gesishub_url: /hub/
        gesisbinder_url: /binder/
        about_url: /about/
        tou_url: /terms_of_use/
        imprint_url: https://www.gesis.org/en/institute/imprint/
        data_protection_url: https://www.gesis.org/en/institute/data-protection/
        gesis_url: https://www.gesis.org/en/home/
        gallery_url: /gallery/
        #help_url: https://www.gesis.org/en/help/
        active: binder

  extraVolumes:
    - name: binder-templates
      configMap:
        name: binder-templates
    - name: binder-templates-gesis
      configMap:
        name: binder-templates-gesis
  extraVolumeMounts:
    - name: binder-templates
      mountPath: /etc/binderhub/templates
    - name: binder-templates-gesis
      mountPath: /etc/binderhub/templates/gesis

  extraConfig:
    11-eventlog: |
      from datetime import datetime
      import jsonschema
      import requests
      from tornado.log import app_log
      from time import sleep
      def emit(self, schema_name, version, event):
          """
          Emit event with given schema / version in a capsule.
          """
          if not self.handlers_maker:
              # If we don't have a handler setup, ignore everything
              return

          if (schema_name, version) not in self.schemas:
              raise ValueError(f'Schema {schema_name} version {version} not registered')
          schema = self.schemas[(schema_name, version)]
          jsonschema.validate(event, schema)

          capsule = {
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'schema': schema_name,
              'version': version
          }
          capsule.update(event)
          self.log.info(capsule)

          # token of binder user in gallery
          headers = {'Authorization': f'Bearer {os.environ["GESIS_API_TOKEN"]}'}
          api_url = os.environ["GESIS_API_URL"]

          # emit function is called after .launch, so we can wait before retry
          # this delay shouldn't effect the user
          retries = 3
          delay = 4
          try:
              for i in range(retries):
                  r = requests.post(api_url, data=capsule, headers=headers)
                  if r.status_code != 201 and i < 2:
                      sleep(delay)
                      delay *= 2
                  else:
                      break
              if r.status_code != 201:
                  app_log.error(f"Error: Event stream failed after {retries} retries for {capsule} with status code {r.status_code}")
          except Exception as e:
              app_log.error(f"Error: Event stream failed for {capsule}: {e}")

      from binderhub.events import EventLog
      EventLog.emit = emit

  service:
    type: NodePort

  cors: &cors
    allowOrigin: '*'

  dind:
    enabled: false

  imageCleaner:
    enabled: false

  jupyterhub:
    custom:
      cors: *cors
    cull:
      # cull every 11 minutes so it is out of phase
      # with the proxy check-routes interval of five minutes
      every: 660
      timeout: 600
      # maxAge is 6 hours: 6 * 3600 = 21600
      maxAge: 21600
    hub:
      # NOTE: hub and proxy must have 1 pod (https://github.com/jupyterhub/jupyterhub/issues/2841#issuecomment-561848594)
      # replicas: 1
      pdb:
        minAvailable: 0
      nodeSelector:
        base: worker  # where database is
      baseUrl: /binder/jupyter/
      db:
        type: postgres
      authenticatePrometheus: false
      extraConfig:
        01-orc: |
          c.KubeSpawner.extra_pod_config.update({'restart_policy': 'Never'})
          if "cookie_options" in c.JupyterHub.tornado_settings:
            c.JupyterHub.tornado_settings["cookie_options"].update({"secure": True})
          else:
            c.JupyterHub.tornado_settings["cookie_options"] = dict(secure=True)
    proxy:
      # NOTE: hub and proxy must have 1 pod (https://github.com/jupyterhub/jupyterhub/issues/2841#issuecomment-561848594)
      # replicas: 1
      pdb:
        minAvailable: 0
      https:
        enabled: false
      service:
        type: NodePort
    singleuser:
      nodeSelector:
        user: worker
      storage:
        extraVolumes:
          - name: etc-jupyter
            configMap:
              name: user-etc-jupyter
          - name: etc-jupyter-templates
            configMap:
              name: user-etc-jupyter-templates
        extraVolumeMounts:
          - name: etc-jupyter
            mountPath: /etc/jupyter
          - name: etc-jupyter-templates
            mountPath: /etc/jupyter/templates

    scheduling:
      userScheduler:
        enabled: false
      podPriority:
        enabled: false
      userPlaceholder:
        enabled: false

imageCleaner:
  enabled: false
  image:
    name: gesiscss/orc-image-cleaner
    tag: 618b48a
  # start deleting images which are not used in x days
  days: 7
  # interval (in seconds) between cleaning processes
  interval: 1800
  # path to check available disk space
  path: /var/lib/docker
  # delete an image at most every 5 seconds
  delay: 10
  # start cleaning when 80% of disk is used (available <= 20%)
  # until it drops below 40% (available >= 60%)
  highLimit: 80
  lowLimit: 40
  # cull images on the host docker
  dockerSocket: /var/run/docker.sock
  # dockerLibDir: /var/lib/docker
  dockerLibDir: /ssd/docker
